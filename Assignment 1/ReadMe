As a matter of fact, to understand the concept of big data, the best way to deal with is to practice, and accordingly, I worked with twitter to apply some fundamentals procedures and calculations for social measurements, where millions of people voluntarily express opinions across any topic imaginable, and this data is incredibly valuable for both research and business. So during this assignment we did the following:
  1. Learning python by Google Python class
  2. Accessing twitter Application Programming Interface (API) using python: So to access twitter live stream, we need to have twitter account and create new app on it using this link by following the instruction in that link. After we need to get "Consumer Key (API Key)", and "Consumer Secret (API Secret)", and "Access token" and also "Access token secret", and need to fill these keys in the code of extract twitter live data code. After we need to run the code for a couple of minutes so we can extract live stream data, and during this task we need to submit the first 20 lines (tweets) from our stream data.
  3. Estimate the public perception of a particular term or phrase:
    Here multiple tasks are requested and they are as the following:
      i. We will compute the sentiment of each tweet based on the sentiment scores of the terms in the tweet. The sentiment of a tweet is equivalent to the sum of the sentiment scores for each term in the tweet. We are provided with text file which contains a list of pre-computed sentiment scores. Each line in the file contains a word or phrase followed by sentiment score. Each word or phrase that is found in a tweet but not found in this file should be given a sentiment score of 0. The data in the tweet file we generated in the second task is represented as JSON, which stands for JavaScript Object Notation. It is a simple format for representing nested structures of data --- lists of lists of dictionaries of lists of ..... items. Each line of output file from task 2 represents a streaming message. Most, but not all, will be tweets. It is straightforward to convert a JSON string into a Python data structure; there is a library to do so called json, we need to import it. So to load json data we will use json.loads for each line that return python data structure (dictionary). So at the end we need to provide a sentiment score for each tweet in the output file from task 2 which, taking in our consideration that we will apply scores just for English tweets where the others we do not have pre-computed sentiment score for words in other languages, the problem that we face that data are really so messy because it is real world data, so we reduce the work by ignoring punctuations such as (? : ! . ; “ @ ‘) from the text of the tweets.
      ii. During this task need to provide a score for the terms that does not appear in the provided pre-computed sentiment score file, so we create four functions and those are the following:
        1. Creating dictionary from provided pre-computed sentiment word score file to load in the memory
        2. Reading text section in each tweet in a string array
        3. Deriving sentiment from each tweet by summing up sentiments of individual word
        4. Derive terms of sentiments that are defined or undefined in the provided pre-computed sentiment word score file.
      iii. In this task we will compute term frequency histogram of the live stream data that we get it in task 2 which can be calculated as the following [#No of occurrences of the term in all tweets] / [#No of occurrences of all terms in all tweets]
           The output should be the word followed by frequency in the whole file
           We create a function to read the tweets and append in the string array of tweets then create term_list that provided the words that occurred in the input file, and total_list which the last is dictionary correspond to the number of occurrence that term over number of total terms. We run in each tweet, each word, checking if they exist in the term_list or not and add it if not then calculating the appearance of it and saving in the total_list. At the end the output will be as word followed by (appearance of this word / total_number_of_words).
  4. Analyze the relationship between location and mood based on a sample of twitter data:
     During this task we had two scripts which provide the happiest state in US and the top ten hash tags which we will compute from our output file and below is the description for each one:
        i. For the happiest state we define dictionary list of states in US with their abbreviations that abbreviation exist in twitter data as json model.
           So first we need to check the place with country code that is US and full name in the place is not none while, sometimes the user does not want to provide information for their own privacy, so we can deal with those tweets only, then we need to for each tweet in each state we need to calculate the sentiment from pre-computed sentiment word score file that is provided and add the value of the score of each tweet that is related to state in list which at the end we will calculate the score for each state divided by (number of tweets) for each and choosing the maximum value between them, this will return the happiest state to us. There is another way to define tweet state and this after checking the country, twitter provide the longitude and latitude of the tweets so we can define area for each state and compare the longitude and latitude for each tweet if it is one of the states, so we can add it to the list and complete the same work accordingly, but this need pre-defined data for each area (longitude and latitude) that unfortunately, we do not have it.
        ii. For top ten hashtags we define two functions one for extracting the hashtags that can be found in json model of twitter in [“entities”][“hashtags”] and the other function is to find the top ten by listing them in frequency list that contains tuples of [hashtag, count] for each and sorting them in reverse way then print out the first ten hashtags with their counts.
  5. More information and details about the assignment can be found in the following link
  6. All the source codes are provided for this assignment in Appendix A and additional files are provided as attachment with the project
